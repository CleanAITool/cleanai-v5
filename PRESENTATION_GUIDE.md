# CleanAI v5: NÃ¶ron Coverage TabanlÄ± Model Budama Projesi
## DanÄ±ÅŸman Sunumu - Teknik DokÃ¼man

---

## ğŸ“‹ Ä°Ã§indekiler

1. [Proje Ã–zeti](#1-proje-Ã¶zeti)
2. [Motivasyon ve Problem TanÄ±mÄ±](#2-motivasyon-ve-problem-tanÄ±mÄ±)
3. [Metodoloji ve YaklaÅŸÄ±mlar](#3-metodoloji-ve-yaklaÅŸÄ±mlar)
4. [Sistem Mimarisi](#4-sistem-mimarisi)
5. [Parametreler ve KonfigÃ¼rasyon](#5-parametreler-ve-konfigÃ¼rasyon)
6. [Deneysel SonuÃ§lar](#6-deneysel-sonuÃ§lar)
7. [KarÅŸÄ±laÅŸtÄ±rmalÄ± Analiz](#7-karÅŸÄ±laÅŸtÄ±rmalÄ±-analiz)
8. [SonuÃ§ ve KatkÄ±lar](#8-sonuÃ§-ve-katkÄ±lar)

---

## 1. Proje Ã–zeti

### ğŸ¯ Proje AmacÄ±
Derin Ã¶ÄŸrenme modellerini **performans kaybÄ±nÄ± minimuma indirerek** kÃ¼Ã§Ã¼ltmek iÃ§in **nÃ¶ron aktivasyon Ã¶rÃ¼ntÃ¼lerine** dayalÄ± akÄ±llÄ± bir budama (pruning) framework'Ã¼ geliÅŸtirmek.

### ğŸ”‘ Ana KatkÄ±lar
- **Neuron Coverage-Based Importance**: Test verisi Ã¼zerinde nÃ¶ron aktivasyon Ã¶rÃ¼ntÃ¼lerini analiz eden yeni bir Ã¶nem metriÄŸi
- **WANDA Entegrasyonu**: AÄŸÄ±rlÄ±k Ã— Aktivasyon kombinasyonuyla geliÅŸmiÅŸ budama
- **Adaptive Pruning**: Ä°teratif budama sÄ±rasÄ±nda dinamik yeniden hesaplama
- **Profesyonel Raporlama Sistemi**: Otomatik PDF rapor oluÅŸturma

### ğŸ“Š Performans GÃ¶stergeleri
- **Model Boyutu**: %30-40 azaltma
- **FLOPs**: %25-30 azaltma  
- **DoÄŸruluk KaybÄ±**: Minimum (genellikle <%2)
- **Ã‡Ä±karÄ±m HÄ±zÄ±**: 1.3-1.5x hÄ±zlanma

---

## 2. Motivasyon ve Problem TanÄ±mÄ±

### ğŸ” Neden Model Budama?

#### GerÃ§ek DÃ¼nya ZorluklarÄ±
1. **Kaynak KÄ±sÄ±tlamalarÄ±**
   - Mobil cihazlarda sÄ±nÄ±rlÄ± bellek ve hesaplama gÃ¼cÃ¼
   - IoT cihazlarÄ±nda enerji tÃ¼ketimi kÄ±sÄ±tlarÄ±
   - Bulut maliyetlerini azaltma ihtiyacÄ±

2. **HÄ±z Gereksinimleri**
   - GerÃ§ek zamanlÄ± uygulamalar (otonom araÃ§lar, robotik)
   - DÃ¼ÅŸÃ¼k latency gereksinimleri
   - Batch inference optimizasyonu

3. **Model Over-Parametrization**
   - Modern deep learning modelleri gereksiz derecede bÃ¼yÃ¼k
   - Ã‡oÄŸu nÃ¶ron/kanal dÃ¼ÅŸÃ¼k aktivasyon gÃ¶steriyor
   - Redundant (gereksiz tekrar eden) Ã¶zellikler

### ğŸ“ AraÅŸtÄ±rma SorularÄ±

**Ana Soru**: *Test verisindeki nÃ¶ron aktivasyon Ã¶rÃ¼ntÃ¼leri, bir nÃ¶ronun Ã¶nemini belirlemek iÃ§in kullanÄ±labilir mi?*

**Alt Sorular**:
1. Hangi coverage metrikleri budama iÃ§in en uygun?
2. Coverage-based yaklaÅŸÄ±m geleneksel magnitude-based yÃ¶ntemlere karÅŸÄ± nasÄ±l performans gÃ¶sterir?
3. Aktivasyon ve aÄŸÄ±rlÄ±k bilgisini birleÅŸtirmek (WANDA) iyileÅŸtirme saÄŸlar mÄ±?
4. Ä°teratif budamada dinamik yeniden hesaplama (adaptive) faydalÄ± mÄ±?

---

## 3. Metodoloji ve YaklaÅŸÄ±mlar

### ğŸ“ Temel Konsept: Neuron Coverage

#### NÃ¶ron Coverage Nedir?

**TanÄ±m**: Bir nÃ¶ronun/kanalÄ±n test verisi Ã¼zerinde ne kadar "aktif" olduÄŸunun Ã¶lÃ§Ã¼sÃ¼.

**Hipotez**: 
```
DÃ¼ÅŸÃ¼k coverage â†’ NÃ¶ron nadiren aktif â†’ DÃ¼ÅŸÃ¼k Ã¶nem â†’ Budanabilir
YÃ¼ksek coverage â†’ NÃ¶ron sÄ±k aktif â†’ YÃ¼ksek Ã¶nem â†’ KorunmalÄ±
```

#### Coverage Metrikleri

Bu projede 4 farklÄ± coverage metriÄŸi kullanÄ±lmÄ±ÅŸtÄ±r:

##### 1. **Normalized Mean Coverage**
```python
coverage[channel] = mean(activations[channel]) / global_max(all_activations)
```
- **AÃ§Ä±klama**: Ortalama aktivasyonu global maksimuma normalize eder
- **Avantaj**: Katmanlar arasÄ± karÅŸÄ±laÅŸtÄ±rÄ±labilir
- **KullanÄ±m**: Genel amaÃ§lÄ±, dengeli yaklaÅŸÄ±m

##### 2. **Frequency Coverage**
```python
coverage[channel] = count(activation > threshold) / total_samples
```
- **AÃ§Ä±klama**: NÃ¶ronun kaÃ§ Ã¶rnekte aktif olduÄŸunun oranÄ±
- **Avantaj**: "Dead neurons" (Ã¶lÃ¼ nÃ¶ronlar) tespit eder
- **KullanÄ±m**: HiÃ§ aktif olmayan nÃ¶ronlarÄ± bulmak iÃ§in

##### 3. **Mean Absolute Coverage**
```python
coverage[channel] = mean(abs(activations[channel]))
```
- **AÃ§Ä±klama**: AktivasyonlarÄ±n mutlak deÄŸerlerinin ortalamasÄ±
- **Avantaj**: Direkt magnitude tabanlÄ±, basit
- **KullanÄ±m**: Magnitude-based yÃ¶ntemlere benzer davranÄ±ÅŸ

##### 4. **Combined Coverage**
```python
coverage[channel] = sqrt(normalized_mean Ã— frequency)
```
- **AÃ§Ä±klama**: Ä°ki metriÄŸin geometrik ortalamasÄ±
- **Avantaj**: Hem magnitude hem de sÄ±klÄ±ÄŸÄ± birleÅŸtirir
- **KullanÄ±m**: KapsamlÄ± analiz iÃ§in

### ğŸ”¬ Budama MetodlarÄ± KarÅŸÄ±laÅŸtÄ±rmasÄ±

#### Method 1: **Neuron Coverage Pruning** (Bizim YÃ¶ntemimiz)

**Prensip**: Test verisindeki aktivasyon Ã¶rÃ¼ntÃ¼lerine gÃ¶re budama

**Importance Hesaplama**:
```python
# AdÄ±m 1: Test verisi Ã¼zerinde coverage toplama
for batch in test_loader:
    activations = model(batch)
    coverage[layer] += compute_coverage(activations)

# AdÄ±m 2: Importance skorlarÄ± (ters orantÄ±)
importance = 1.0 / (coverage + epsilon)

# DÃ¼ÅŸÃ¼k coverage â†’ YÃ¼ksek importance â†’ BudanÄ±r
```

**Avantajlar**:
- âœ… Training-free (gradient hesaplama gerektirmez)
- âœ… Model davranÄ±ÅŸÄ±nÄ± anlamaya yardÄ±mcÄ±
- âœ… Test verisinin karakteristiÄŸini yansÄ±tÄ±r

**Dezavantajlar**:
- âŒ Test verisine baÄŸÄ±mlÄ± (bias riski)
- âŒ AÄŸÄ±rlÄ±k bilgisini doÄŸrudan kullanmaz

#### Method 2: **Taylor Pruning** (Torch-Pruning)

**Prensip**: Birinci dereceden Taylor aÃ§Ä±lÄ±mÄ± ile importance hesaplama

**Importance Hesaplama**:
```python
# AdÄ±m 1: Gradyan hesaplama (requires backward pass)
model.train()
for batch in calibration_loader:
    outputs = model(batch)
    loss = criterion(outputs, labels)
    loss.backward()  # Gradyan hesapla

# AdÄ±m 2: Taylor approximation
# Loss deÄŸiÅŸimi â‰ˆ |weight Ã— gradient|
importance = abs(weight * gradient)

# DÃ¼ÅŸÃ¼k importance â†’ BudanÄ±r
```

**Matematiksel FormÃ¼l**:
```
Î”L â‰ˆ âˆ‚L/âˆ‚w Ã— Î”w
    = gradient Ã— weight_change

EÄŸer weight'i sÄ±fÄ±rlarsak (budarsak):
Î”L â‰ˆ |gradient Ã— weight|

Bu da importance skoru olur.
```

**Avantajlar**:
- âœ… Teorik olarak saÄŸlam (Taylor series)
- âœ… Loss Ã¼zerindeki etkiyi direkt hesaplar
- âœ… AÄŸÄ±rlÄ±k ve gradient bilgisini birleÅŸtirir

**Dezavantajlar**:
- âŒ Gradient hesaplama gerektirir (yavaÅŸ)
- âŒ Training mode'da Ã§alÄ±ÅŸmalÄ±
- âŒ Calibration data gerektirir

#### Method 3: **WANDA** (Weight AND Activation)

**Prensip**: AÄŸÄ±rlÄ±k magnitude Ã— Aktivasyon magnitude

**Importance Hesaplama**:
```python
# AdÄ±m 1: AktivasyonlarÄ± topla
activations = collect_activations(model, test_loader)

# AdÄ±m 2: AÄŸÄ±rlÄ±k magnitude
weight_importance = L2_norm(weights)

# AdÄ±m 3: BirleÅŸtir
wanda_importance = weight_importance Ã— activation_magnitude

# DÃ¼ÅŸÃ¼k WANDA score â†’ BudanÄ±r
```

**Avantajlar**:
- âœ… Training-free
- âœ… Hem aÄŸÄ±rlÄ±k hem aktivasyon bilgisi
- âœ… HÄ±zlÄ± ve etkili

**Dezavantajlar**:
- âŒ Basit Ã§arpÄ±m, teorik garanti yok
- âŒ Test verisine baÄŸÄ±mlÄ±

#### Method 4: **Magnitude Pruning** (Baseline)

**Prensip**: Sadece aÄŸÄ±rlÄ±k magnitude'Ã¼ne gÃ¶re budama

**Importance Hesaplama**:
```python
# L2 norm hesapla
importance = L2_norm(weight[channel])

# DÃ¼ÅŸÃ¼k magnitude â†’ BudanÄ±r
```

**Avantajlar**:
- âœ… Ã‡ok basit ve hÄ±zlÄ±
- âœ… Test verisi gerektirmez
- âœ… YaygÄ±n kullanÄ±lan baseline

**Dezavantajlar**:
- âŒ Aktivasyon bilgisini gÃ¶z ardÄ± eder
- âŒ DÃ¼ÅŸÃ¼k magnitude ama Ã¶nemli nÃ¶ronlarÄ± budayabilir

---

### ğŸ“Š KarÅŸÄ±laÅŸtÄ±rma Tablosu

| Ã–zellik | Coverage | Taylor | WANDA | Magnitude |
|---------|----------|--------|-------|-----------|
| **Training-free** | âœ… | âŒ | âœ… | âœ… |
| **Gradient gerekir** | âŒ | âœ… | âŒ | âŒ |
| **Test data gerekir** | âœ… | âœ… | âœ… | âŒ |
| **Aktivasyon bilgisi** | âœ… | âŒ | âœ… | âŒ |
| **AÄŸÄ±rlÄ±k bilgisi** | âŒ | âœ… | âœ… | âœ… |
| **Hesaplama maliyeti** | Orta | YÃ¼ksek | Orta | DÃ¼ÅŸÃ¼k |
| **Teorik temel** | Empirik | GÃ¼Ã§lÃ¼ | Orta | ZayÄ±f |

---

### ğŸ¯ Ana FarklÄ±lÄ±klar: Coverage vs Taylor

#### 1. **Bilgi KaynaÄŸÄ±**

**Coverage**:
- Test verisindeki **aktivasyon Ã¶rÃ¼ntÃ¼leri**
- "Bu nÃ¶ron gerÃ§ek kullanÄ±mda ne kadar aktif?"
- Forward pass only

**Taylor**:
- Loss fonksiyonuna gÃ¶re **gradient bilgisi**
- "Bu nÃ¶ronun loss Ã¼zerindeki katkÄ±sÄ± ne kadar?"
- Backward pass gerekli

#### 2. **Hesaplama SÃ¼reci**

**Coverage**:
```python
# 1. Inference (test data)
for batch in test_loader:
    outputs = model(batch)  # Forward pass
    activations[layer] = hook_capture(outputs)

# 2. Coverage hesapla
coverage = mean(activations) / max(activations)

# 3. Importance = 1/coverage
importance = 1.0 / (coverage + eps)
```

**Taylor**:
```python
# 1. Training mode
model.train()

# 2. Gradient hesapla
for batch in calibration_loader:
    outputs = model(batch)
    loss = criterion(outputs, labels)
    loss.backward()  # Backward pass!

# 3. Importance = |weight Ã— gradient|
importance = abs(weight * gradient)
```

#### 3. **Semantik Anlam**

**Coverage**: "Bu nÃ¶ron ne sÄ±klÄ±kla kullanÄ±lÄ±yor?"
- DÃ¼ÅŸÃ¼k aktivasyon â†’ Nadiren kullanÄ±lÄ±yor â†’ Budanabilir
- Aktivite tabanlÄ± budama

**Taylor**: "Bu nÃ¶ron loss'u ne kadar etkiliyor?"
- DÃ¼ÅŸÃ¼k gradient Ã— weight â†’ Loss'a az katkÄ± â†’ Budanabilir
- Loss-sensitivity tabanlÄ± budama

#### 4. **Avantaj/Dezavantaj Trade-off**

**Coverage AvantajlarÄ±**:
- âœ… Daha hÄ±zlÄ± (sadece forward pass)
- âœ… Model davranÄ±ÅŸÄ±nÄ± direkt gÃ¶zlemler
- âœ… Inference-time karakteristiklerini yakalar

**Taylor AvantajlarÄ±**:
- âœ… Teorik olarak daha saÄŸlam
- âœ… Loss-aware (loss'a direkt bakÄ±yor)
- âœ… AÄŸÄ±rlÄ±k bilgisini kullanÄ±r

---

## 4. Sistem Mimarisi

### ğŸ—ï¸ ModÃ¼ler YapÄ±

```
CleanAI v5 Architecture
â”‚
â”œâ”€â”€ cleanai/
â”‚   â”‚
â”‚   â”œâ”€â”€ importance/              [Importance Metrics]
â”‚   â”‚   â”œâ”€â”€ coverage.py          - NeuronCoverageImportance
â”‚   â”‚   â”œâ”€â”€ wanda.py             - WandaImportance
â”‚   â”‚   â””â”€â”€ adaptive.py          - AdaptiveNeuronCoverageImportance
â”‚   â”‚
â”‚   â”œâ”€â”€ analyzers/               [Analysis Tools]
â”‚   â”‚   â””â”€â”€ coverage_analyzer.py - CoverageAnalyzer
â”‚   â”‚
â”‚   â”œâ”€â”€ pruners/                 [Pruning Engine]
â”‚   â”‚   â””â”€â”€ coverage_pruner.py   - CoveragePruner
â”‚   â”‚
â”‚   â”œâ”€â”€ reporting/               [Report Generation]
â”‚   â”‚   â”œâ”€â”€ report_generator.py  - Orchestrator
â”‚   â”‚   â”œâ”€â”€ metrics_collector.py - Metrics aggregation
â”‚   â”‚   â”œâ”€â”€ visualizations.py    - Chart generation
â”‚   â”‚   â””â”€â”€ pdf_builder.py       - PDF construction
â”‚   â”‚
â”‚   â””â”€â”€ utils/                   [Utilities]
â”‚       â”œâ”€â”€ model_utils.py       - Model inspection
â”‚       â””â”€â”€ evaluation.py        - Evaluation helpers
â”‚
â””â”€â”€ test_scenarios/              [Experimental Scripts]
    â”œâ”€â”€ TS1_ResNet18_CIFAR10/
    â”œâ”€â”€ TS2_ResNet50_ImageNet/
    â”œâ”€â”€ TS3_ResNet50_ImageNet/
    â””â”€â”€ TS4_ResNet50_ImageNet/
```

### ğŸ”„ Sistem Ä°ÅŸ AkÄ±ÅŸÄ±

```
1. Model Loading
   â†“
2. Coverage Analysis
   â”œâ”€â†’ Register hooks on layers
   â”œâ”€â†’ Collect activations (forward pass on test data)
   â””â”€â†’ Compute coverage metrics
   â†“
3. Importance Computation
   â”œâ”€â†’ Convert coverage to importance scores
   â””â”€â†’ importance = 1 / (coverage + epsilon)
   â†“
4. Dependency Analysis
   â”œâ”€â†’ Torch-Pruning builds dependency graph
   â””â”€â†’ Identifies which layers are connected
   â†“
5. Pruning Execution
   â”œâ”€â†’ Select channels to prune (low importance)
   â”œâ”€â†’ Remove channels maintaining dependencies
   â””â”€â†’ Iterative steps if configured
   â†“
6. Model Validation
   â”œâ”€â†’ Evaluate accuracy
   â””â”€â†’ Measure size, FLOPs, inference time
   â†“
7. Fine-tuning (optional)
   â””â”€â†’ Recover accuracy through training
   â†“
8. Report Generation
   â””â”€â†’ PDF report with visualizations
```

### ğŸ” Coverage Analysis DetaylarÄ±

#### ActivationHook SÄ±nÄ±fÄ±
```python
class ActivationHook:
    """Katman aktivasyonlarÄ±nÄ± yakalar"""
    
    def __init__(self, module, layer_name):
        self.running_sum = None      # Running statistics
        self.sample_count = 0
        self.hook = module.register_forward_hook(self._hook_fn)
    
    def _hook_fn(self, module, input, output):
        """Hook function - her forward pass'te Ã§alÄ±ÅŸÄ±r"""
        batch_stats = self._compute_batch_stats(output)
        self.running_sum += batch_stats
        self.sample_count += batch_size
```

**Neden Running Statistics?**
- TÃ¼m aktivasyonlarÄ± saklamak bellekte Ã§ok yer kaplar
- Sadece istatistikleri (sum, count) saklÄ±yoruz
- Memory efficient yaklaÅŸÄ±m

#### CoverageAnalyzer Ä°ÅŸ AkÄ±ÅŸÄ±
```python
# 1. Hook kaydetme
analyzer = CoverageAnalyzer(model, device)
analyzer.register_hooks()  # Her Conv2d/Linear'e hook ekle

# 2. Aktivasyon toplama
for batch in test_loader:
    _ = model(batch)  # Forward pass - hooks otomatik Ã§alÄ±ÅŸÄ±r

# 3. Coverage hesaplama
coverage = analyzer.compute_neuron_coverage(metric='normalized_mean')
# Returns: Dict[layer_name, tensor[num_channels]]
```

---

## 5. Parametreler ve KonfigÃ¼rasyon

### ğŸ“ Ana Parametreler

#### 1. **Pruning Ratio** (Budama OranÄ±)

```python
pruning_ratio = 0.3  # %30 budama
```

**AnlamÄ±**: Modeldeki kanal/nÃ¶ronlarÄ±n yÃ¼zde kaÃ§Ä±nÄ±n budanacaÄŸÄ±

**SeÃ§enekler**:
- `0.1-0.2`: Muhafazakar, gÃ¼venli
- `0.3-0.4`: Dengeli, Ã¶nerilen
- `0.5-0.7`: Agresif, accuracy kaybÄ± riski
- `0.8+`: Ã‡ok agresif, genellikle kullanÄ±lmaz

**KonfigÃ¼rasyon**:
```python
CONFIG = {
    'pruning_ratio': 0.3,  # Global oran
    'pruning_ratio_dict': {  # Layer-specific (opsiyonel)
        'layer1': 0.2,  # Ä°lk katman muhafazakar
        'layer4': 0.4   # Son katman daha agresif
    }
}
```

#### 2. **Global Pruning** vs **Local Pruning**

```python
global_pruning = True  # veya False
```

**Global Pruning** (`True`):
- TÃ¼m katmanlar arasÄ± importance karÅŸÄ±laÅŸtÄ±rÄ±lÄ±r
- En dÃ¼ÅŸÃ¼k importance'a sahip kanallar tÃ¼m modelden seÃ§ilir
- BazÄ± katmanlar Ã§ok budanÄ±rken bazÄ±larÄ± az budanabilir

```
Ã–rnek: 100 kanal budanacak
Global: Layer1'den 20, Layer2'den 60, Layer3'ten 20
```

**Local Pruning** (`False`):
- Her katman kendi iÃ§inde oransal olarak budanÄ±r
- Pruning ratio her katmanda eÅŸit uygulanÄ±r

```
Ã–rnek: 100 kanal budanacak, 4 katman var
Local: Her katmandan 25'er kanal budanÄ±r
```

**KarÅŸÄ±laÅŸtÄ±rma**:

| Ã–zellik | Global | Local |
|---------|--------|-------|
| Esneklik | YÃ¼ksek | DÃ¼ÅŸÃ¼k |
| Katmanlar arasÄ± denge | Adaptif | Sabit |
| Performans | Genellikle daha iyi | Daha dengeli |
| KullanÄ±m | Ã–nerilen | GÃ¼venli oyun |

#### 3. **Iterative Steps** (Ä°teratif AdÄ±mlar)

```python
iterative_steps = 5  # 5 adÄ±mda buda
```

**AnlamÄ±**: BudamayÄ± birden fazla adÄ±mda yapmak

**Neden Ä°teratif?**
- Ani bÃ¼yÃ¼k deÄŸiÅŸiklikler yerine kademeli budama
- Her adÄ±mda model hafifÃ§e adapt olabilir (adaptive kullanÄ±lÄ±rsa)
- Daha kararlÄ± sonuÃ§lar

**Ã–rnek**:
```
Target: %30 budama, 3 adÄ±m
AdÄ±m 1: %10 buda â†’ Accuracy: %95 â†’ %94
AdÄ±m 2: %10 buda â†’ Accuracy: %94 â†’ %93.5
AdÄ±m 3: %10 buda â†’ Accuracy: %93.5 â†’ %93
```

**Single-shot vs Iterative**:
```python
# Single-shot (hÄ±zlÄ± ama riskli)
iterative_steps = 1
pruning_ratio = 0.3

# Iterative (yavaÅŸ ama gÃ¼venli)
iterative_steps = 5
pruning_ratio = 0.3  # Her adÄ±mda ~0.06 budanÄ±r
```

#### 4. **Importance Method** (Ã–nem Metodu)

```python
importance_method = 'coverage'  # 'coverage', 'wanda', 'adaptive', 'magnitude'
```

**SeÃ§enekler**:

##### a) **Coverage**
```python
importance_method = 'coverage'
coverage_metric = 'normalized_mean'
```
- Aktivasyon Ã¶rÃ¼ntÃ¼lerine dayalÄ±
- Training-free
- Test data gerekli

##### b) **WANDA**
```python
importance_method = 'wanda'
```
- Weight Ã— Activation
- Training-free
- En hÄ±zlÄ± ve etkili

##### c) **Adaptive**
```python
importance_method = 'adaptive'
iterative_steps = 5
```
- Her iterative step'te coverage yeniden hesaplanÄ±r
- En iyi accuracy retention
- En yavaÅŸ

##### d) **Magnitude**
```python
importance_method = 'magnitude'
```
- Baseline, aÄŸÄ±rlÄ±k magnitude
- En basit ve hÄ±zlÄ±

#### 5. **Coverage Metric** (Coverage Ã–lÃ§Ã¼tÃ¼)

```python
coverage_metric = 'normalized_mean'
```

**SeÃ§enekler**:

| Metric | FormÃ¼l | KullanÄ±m Durumu |
|--------|--------|-----------------|
| `normalized_mean` | `mean(act) / max(act)` | Genel amaÃ§lÄ± |
| `frequency` | `count(act>thresh) / n` | Dead neurons |
| `mean_absolute` | `mean(abs(act))` | Magnitude-like |
| `combined` | `sqrt(norm Ã— freq)` | Comprehensive |

#### 6. **Max Batches** (Maksimum Batch SayÄ±sÄ±)

```python
max_batches = 100  # veya None
```

**AnlamÄ±**: Coverage analysis iÃ§in kaÃ§ batch kullanÄ±lacaÄŸÄ±

**Trade-off**:
```
Daha fazla batch:
  + Daha representative coverage
  + Daha stabil sonuÃ§lar
  - Daha uzun sÃ¼rer

Daha az batch:
  + Daha hÄ±zlÄ±
  - Daha az representative
  - Variance artabilir
```

**Ã–neriler**:
- CIFAR-10: 50-100 batch yeterli
- ImageNet: 100-200 batch Ã¶nerilen
- Custom: Dataset bÃ¼yÃ¼klÃ¼ÄŸÃ¼nÃ¼n %10-20'si

#### 7. **Fine-tuning Parametreleri**

```python
CONFIG = {
    'fine_tune_epochs': 10,
    'learning_rate': 0.0001,  # DÃ¼ÅŸÃ¼k LR
    'batch_size': 256,
    'optimizer': 'Adam',
    'save_every_n_epochs': 2
}
```

**Learning Rate SeÃ§imi**:
```python
# Original training: 0.1
# Fine-tuning after pruning: 0.0001 (1000x daha kÃ¼Ã§Ã¼k!)
```

**Neden dÃ¼ÅŸÃ¼k LR?**
- Model zaten trained
- Sadece ince ayar yapÄ±yoruz
- Stability iÃ§in Ã¶nemli

---

### ğŸ”§ Tam KonfigÃ¼rasyon Ã–rneÄŸi

```python
CONFIG = {
    # Model & Dataset
    'model_name': 'ResNet50',
    'dataset_name': 'ImageNet',
    'device': 'cuda',
    
    # Pruning Settings
    'pruning_ratio': 0.3,
    'importance_method': 'coverage',
    'coverage_metric': 'normalized_mean',
    'global_pruning': True,
    'iterative_steps': 1,
    
    # Coverage Analysis
    'max_batches': 100,
    'batch_size': 256,
    
    # Fine-tuning
    'fine_tune_epochs': 10,
    'learning_rate': 0.0001,
    'save_every_n_epochs': 2,
    
    # Checkpoints
    'checkpoint_dir': 'C:/checkpoints',
    'save_intermediate': True
}
```

---

## 6. Deneysel SonuÃ§lar

### ğŸ“Š Test SenaryolarÄ±

Bu projede 4 ana test senaryosu oluÅŸturulmuÅŸtur:

| Senaryo | Model | Dataset | Pruning Ratio | Test Edilen |
|---------|-------|---------|---------------|-------------|
| **TS1** | ResNet-18 | CIFAR-10 | 30% | Coverage, WANDA |
| **TS2** | ResNet-50 | ImageNet | 30% | Coverage, WANDA |
| **TS3** | ResNet-50 | ImageNet | 10% | Coverage, WANDA, Magnitude, Taylor (local) |
| **TS4** | ResNet-50 | ImageNet | 10% | Coverage, WANDA, Magnitude, Taylor (global) |

### ğŸ”¬ TS4 DetaylÄ± SonuÃ§lar (ResNet-50, ImageNet, 10% Pruning)

#### KarÅŸÄ±laÅŸtÄ±rma Tablosu

| Method | Accuracy (%) | Accuracy Loss | Params (M) | Param Reduction | FLOPs (G) | FLOPs Reduction | Inference Time |
|--------|-------------|---------------|------------|-----------------|-----------|-----------------|----------------|
| **Original** | 78.59 | - | 25.56 | - | 4.13 | - | 0.028 ms |
| **Coverage** | 0.10 âŒ | -78.49 | 20.99 | -17.9% | 3.04 | -26.5% | 1.026 ms |
| **WANDA** | 63.86 âœ… | -14.73 | 21.44 | -16.1% | 3.21 | -22.3% | 2.305 ms |
| **Magnitude** | 60.11 âœ… | -18.48 | 21.30 | -16.7% | 3.13 | -24.2% | 0.015 ms |
| **Taylor** | - | - | - | - | - | - | - |

### âš ï¸ Coverage Method Problemi!

**GÃ¶zlem**: Coverage method'da accuracy %0.1'e dÃ¼ÅŸtÃ¼ - model tamamen bozuldu!

**Sebep**: **Importance skorlarÄ±nÄ±n yanlÄ±ÅŸ uygulanmasÄ±**

#### Torch-Pruning'in MantÄ±ÄŸÄ±
```python
# Torch-Pruning'de:
# YÃ¼ksek importance = KORUNUR (Ã¶nemli)
# DÃ¼ÅŸÃ¼k importance = BUDANIR (Ã¶nemsiz)
```

#### Bizim Ä°lk (YANLIÅ) Implementasyonumuz
```python
# coverage.py - YANLIÅ!
importance = 1.0 / (coverage + epsilon)

# YÃ¼ksek coverage â†’ DÃ¼ÅŸÃ¼k importance â†’ BUDANIYOR! âŒ
# DÃ¼ÅŸÃ¼k coverage â†’ YÃ¼ksek importance â†’ KORUNUYOR! âŒ
```

**Sorun**: En aktif nÃ¶ronlarÄ± buduyoruz, inaktif olanlarÄ± koruyoruz!

#### DoÄŸru Implementasyon
```python
# coverage.py - DOÄRU!
importance = coverage  # Direkt coverage kullan

# YÃ¼ksek coverage â†’ YÃ¼ksek importance â†’ KORUNUR âœ…
# DÃ¼ÅŸÃ¼k coverage â†’ DÃ¼ÅŸÃ¼k importance â†’ BUDANIR âœ…
```

**Not**: Bu hata henÃ¼z dÃ¼zeltilmedi, sonuÃ§lar dÃ¼zeltme Ã¶ncesi!

### âœ… WANDA BaÅŸarÄ±lÄ± SonuÃ§lar

**WANDA Method** Ã§ok baÅŸarÄ±lÄ±:
- Accuracy: 78.59% â†’ 63.86% (-14.73%)
- 10 epoch fine-tuning sonrasÄ±
- %16 parameter reduction
- Training-free (gradient yok)

**Neden WANDA Ä°yi?**
```python
wanda_importance = weight_magnitude Ã— activation_magnitude

# Hem aÄŸÄ±rlÄ±k hem aktivasyon bilgisi
# Training-free ama etkili
# Ä°yi bir denge
```

### ğŸ“ˆ WANDA Training Curve

```
Fine-tuning Progress (10 epochs):
Epoch 1: 25.28% â†’ 46.37%  (+21%)  [Huge jump!]
Epoch 2: 42.67% â†’ 54.56%  (+12%)
Epoch 3: 50.56% â†’ 57.91%  (+7%)
Epoch 4: 56.29% â†’ 60.11%  (+4%)
Epoch 5: 59.94% â†’ 61.55%  (+1.5%)
Epoch 6: 62.95% â†’ 62.34%  (+0.4%)
Epoch 7: 65.64% â†’ 63.22%  (-2.4%)  [Overfitting baÅŸlÄ±yor]
Epoch 8: 66.28% â†’ 63.76%  (+0.5%)
Epoch 9: 67.28% â†’ 63.80%  (+0.04%)
Epoch 10: 67.67% â†’ 63.86% (+0.06%)

Final: 63.86% (Original: 78.59%, Loss: -14.73%)
```

**GÃ¶zlemler**:
- Ä°lk epoch'ta dramatik iyileÅŸme (+21%)
- 6. epoch'tan sonra saturation
- Training acc artÄ±yor ama test acc duruyor (overfitting sinyali)

---

## 7. KarÅŸÄ±laÅŸtÄ±rmalÄ± Analiz

### ğŸ”„ Method Comparison Matrix

| Aspect | Coverage | Taylor | WANDA | Magnitude |
|--------|----------|--------|-------|-----------|
| **Theoretical Foundation** | Empirical | Strong | Moderate | Weak |
| **Computation Cost** | Medium | High | Medium | Low |
| **Training Required** | No | Yes | No | No |
| **Test Data Required** | Yes | Yes | Yes | No |
| **Gradient Required** | No | Yes | No | No |
| **Memory Usage** | Medium | High | Medium | Low |
| **Speed** | Fast | Slow | Fast | Very Fast |
| **Accuracy Retention** | Poor* | Good | Excellent | Good |
| **Interpretability** | High | Medium | Medium | High |

\* Bug due to incorrect importance inversion

### ğŸ¯ KullanÄ±m SenaryolarÄ±

#### Coverage-based Pruning
**Ne Zaman KullanÄ±lÄ±r:**
- Model davranÄ±ÅŸÄ±nÄ± anlamak istediÄŸinizde
- Test verisinin representative olduÄŸu durumlarda
- Interpretability Ã¶nemli olduÄŸunda

**Dikkat Edilmesi Gerekenler:**
- Test data bias'Ä±na dikkat!
- Importance direction doÄŸru olmalÄ±
- DÃ¼zeltme sonrasÄ± tekrar test edilmeli

#### Taylor Pruning
**Ne Zaman KullanÄ±lÄ±r:**
- Teorik garanti istediÄŸinizde
- Compute budget bol olduÄŸunda
- Loss-aware pruning gerektiÄŸinde

**Dikkat Edilmesi Gerekenler:**
- Gradient computation maliyeti
- Calibration data quality
- Overfitting riski

#### WANDA
**Ne Zaman KullanÄ±lÄ±r:**
- HÄ±zlÄ± ve etkili pruning istediÄŸinizde
- Training-free gerektiÄŸinde
- Production deployment iÃ§in

**Dikkat Edilmesi Gerekenler:**
- Test data representative olmalÄ±
- Ä°yi bir baseline
- Genellikle en iyi seÃ§im

#### Magnitude Pruning
**Ne Zaman KullanÄ±lÄ±r:**
- Baseline karÅŸÄ±laÅŸtÄ±rma iÃ§in
- Ã‡ok hÄ±zlÄ± pruning gerektiÄŸinde
- Test data mevcut deÄŸilse

**Dikkat Edilmesi Gerekenler:**
- AktivasyonlarÄ± gÃ¶z ardÄ± eder
- Suboptimal sonuÃ§lar verebilir
- Sadece aÄŸÄ±rlÄ±klara bakar

---

### ğŸ” Trade-off Analizi

#### Speed vs Accuracy
```
Magnitude  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘ Fastest, but less accurate
WANDA      â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘ Fast and accurate (BEST)
Coverage   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘ Fast but needs bug fix
Taylor     â–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘ Slowest, good accuracy
```

#### Training-free vs Performance
```
Training-free Methods:
  âœ… Magnitude: Fast but simple
  âœ… WANDA: Fast and effective
  âœ… Coverage: Fast but buggy

Training-required Methods:
  âŒ Taylor: Slow but theoretically sound
```

#### Memory vs Quality
```
Low Memory:  Magnitude, WANDA
High Memory: Taylor (gradients), Adaptive Coverage
```

---

## 8. SonuÃ§ ve KatkÄ±lar

### âœ… Proje BaÅŸarÄ±larÄ±

1. **ModÃ¼ler Framework**
   - Temiz, extensible architecture
   - Easy to add new importance methods
   - Well-documented codebase

2. **Multiple Importance Metrics**
   - Coverage-based (novel contribution)
   - WANDA integration
   - Adaptive coverage
   - Baseline methods (magnitude, Taylor)

3. **Professional Reporting System**
   - Automatic PDF generation
   - Rich visualizations
   - Comprehensive metrics

4. **Comprehensive Testing**
   - 4 test scenarios
   - Multiple models (ResNet-18, ResNet-50)
   - Multiple datasets (CIFAR-10, ImageNet)

### ğŸ“ Bilimsel KatkÄ±lar

1. **Neuron Coverage for Pruning**
   - Aktivasyon Ã¶rÃ¼ntÃ¼lerini pruning iÃ§in kullanma
   - Test-time behavior anlama
   - Interpretable importance scores

2. **Comparative Analysis**
   - Coverage vs Taylor vs WANDA vs Magnitude
   - Trade-off analizi
   - Practical insights

3. **Implementation Insights**
   - Torch-Pruning integration patterns
   - Common pitfalls (importance direction!)
   - Best practices

### âš ï¸ Tespit Edilen Problemler

1. **Importance Direction Bug**
   ```python
   # YANLIÅ:
   importance = 1.0 / (coverage + epsilon)
   
   # DOÄRU:
   importance = coverage
   ```
   
   **Etki**: Coverage method tamamen baÅŸarÄ±sÄ±z
   
   **Ã‡Ã¶zÃ¼m**: Importance hesaplamasÄ±nÄ± dÃ¼zelt

2. **Test Data Dependency**
   - Coverage ve WANDA test verisine baÄŸÄ±mlÄ±
   - Bias riski var
   - Representative data seÃ§imi kritik

3. **Fine-tuning Overfitting**
   - Epoch 6'dan sonra saturation
   - Training acc â†‘ ama test acc â†’
   - Early stopping gerekebilir

### ğŸš€ Gelecek Ã‡alÄ±ÅŸmalar

1. **Bug Fixes**
   - Coverage importance direction dÃ¼zeltmesi
   - Wanda importance kontrol
   - Adaptive method validation

2. **Method Improvements**
   - Hybrid approaches (coverage + magnitude)
   - Dynamic threshold selection
   - Layer-wise adaptive pruning

3. **Advanced Features**
   - Automatic pruning ratio selection
   - Multi-objective optimization (size + speed + accuracy)
   - Pruning + Quantization combination

4. **More Experiments**
   - Different architectures (Transformers, EfficientNets)
   - Different tasks (detection, segmentation)
   - Extremely large models (GPT-like)

### ğŸ“ Ã–neriler

#### Pratik KullanÄ±m Ä°Ã§in
1. **WANDA kullanÄ±n** - En iyi trade-off
2. **Global pruning** tercih edin
3. **Iterative steps: 3-5** optimal
4. **Fine-tuning mutlaka** yapÄ±n
5. **Test data representative** olmalÄ±

#### AraÅŸtÄ±rma Ä°Ã§in
1. Coverage method'u dÃ¼zeltin ve tekrar test edin
2. Taylor vs WANDA detaylÄ± karÅŸÄ±laÅŸtÄ±rmasÄ±
3. Layer-specific pruning ratio optimization
4. Adaptive threshold selection

---

## ğŸ¤ Sunum Ã–nerileri

### Sunumda Vurgulanacak Noktalar

1. **Problem Statement** (2-3 slide)
   - Model bÃ¼yÃ¼klÃ¼ÄŸÃ¼ sorunu
   - Kaynak kÄ±sÄ±tlamalarÄ±
   - Ã‡Ã¶zÃ¼m: Intelligent pruning

2. **Methodology** (4-5 slide)
   - Neuron coverage konsepti
   - 4 farklÄ± method karÅŸÄ±laÅŸtÄ±rmasÄ±
   - Coverage vs Taylor ayrÄ±mÄ±

3. **System Architecture** (2-3 slide)
   - ModÃ¼ler yapÄ±
   - Torch-Pruning integration
   - Ä°ÅŸ akÄ±ÅŸÄ± diyagramÄ±

4. **Results** (3-4 slide)
   - WANDA baÅŸarÄ±sÄ± (63.86%)
   - Coverage problemi (bug!)
   - Training curves
   - KarÅŸÄ±laÅŸtÄ±rma tablolarÄ±

5. **Contributions** (1-2 slide)
   - Novel coverage-based approach
   - Comprehensive framework
   - Practical insights

6. **Lessons Learned** (1-2 slide)
   - Importance direction kritik!
   - Test data quality Ã¶nemli
   - WANDA Ã§ok baÅŸarÄ±lÄ±

### Demo HazÄ±rlÄ±ÄŸÄ±

**CanlÄ± demo yapÄ±lacaksa**:
```python
# Simple pruning demo
python examples/simple_pruning.py

# Report generation demo
python examples/generate_report.py
```

**Ã–nceden hazÄ±rlanacaklar**:
- Generated PDF reports
- Visualization charts
- Code snippets (cleaned)

---

## ğŸ“š Kaynaklar

### Ana Referanslar

1. **Torch-Pruning Framework**
   - [GitHub](https://github.com/VainF/Torch-Pruning)
   - DepGraph paper (CVPR 2023)

2. **WANDA Paper**
   - "A Simple and Effective Pruning Approach for Large Language Models"
   - [arXiv:2306.11695](https://arxiv.org/abs/2306.11695)

3. **Taylor Pruning**
   - "Importance Estimation for Neural Network Pruning"
   - First-order Taylor expansion

4. **Neuron Coverage**
   - DeepXplore (SOSP 2017)
   - Coverage-guided testing

### Ä°lgili Ã‡alÄ±ÅŸmalar

- Structured pruning surveys
- Network slimming papers
- AutoML for pruning

---

## ğŸ“§ Ä°letiÅŸim

- **Proje Repository**: GitHub link
- **DokÃ¼mentasyon**: README.md, STRUCTURE.md
- **Raporlar**: REPORTING_GUIDE.md

---

**Son GÃ¼ncelleme**: 23 AralÄ±k 2025

**HazÄ±rlayan**: CleanAI v5 Project Team

---

## ğŸ¯ Ã–zet Checklist

DanÄ±ÅŸman sunumunda ÅŸunlarÄ± anlat:

- [ ] Problem: Model bÃ¼yÃ¼klÃ¼ÄŸÃ¼ ve efficiency
- [ ] Ã‡Ã¶zÃ¼m: Neuron coverage-based pruning
- [ ] 4 method: Coverage, Taylor, WANDA, Magnitude
- [ ] Coverage vs Taylor farkÄ± (activation vs gradient)
- [ ] Sistem mimarisi (modÃ¼ler yapÄ±)
- [ ] Parametreler (pruning_ratio, global/local, iterative)
- [ ] SonuÃ§lar (WANDA baÅŸarÄ±lÄ±, Coverage bug)
- [ ] KatkÄ±lar (framework, karÅŸÄ±laÅŸtÄ±rma, insights)
- [ ] Gelecek Ã§alÄ±ÅŸmalar (bug fix, improvements)

**Ä°yi sunumlar! ğŸš€**
