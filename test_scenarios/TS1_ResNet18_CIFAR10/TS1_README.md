# Test Scenario TS1: ResNet-18 CIFAR-10 Pruning Analysis

Bu test senaryosu, ResNet-18 modelinin CIFAR-10 dataseti Ã¼zerinde iki farklÄ± pruning yÃ¶ntemi (Neuron Coverage ve WANDA) ile budanmasÄ±nÄ± ve sonuÃ§larÄ±n karÅŸÄ±laÅŸtÄ±rÄ±lmasÄ±nÄ± iÃ§erir.

## ğŸ“‹ Senaryo Bilgileri

- **Test Senaryosu No**: TS1
- **Model**: ResNet-18
- **Dataset**: CIFAR-10
- **Pruning OranÄ±**: 20% (0.2)
- **Pruning YÃ¶ntemleri**: 
  - Neuron Coverage
  - WANDA (Weight AND Activation)

## ğŸ“ KlasÃ¶r YapÄ±sÄ±

```
C:\source\
â”œâ”€â”€ downloaded_models/                    # Pretrained modeller
â”œâ”€â”€ downloaded_datasets/                  # CIFAR-10 dataset
â”œâ”€â”€ checkpoints\
â”‚   â”œâ”€â”€ TS1/                             # Script 1 checkpointleri
â”‚   â”‚   â”œâ”€â”€ ResNet18_CIFAR10_pretrained.pth
â”‚   â”‚   â”œâ”€â”€ ResNet18_CIFAR10_FT_epoch5.pth
â”‚   â”‚   â”œâ”€â”€ ResNet18_CIFAR10_FT_epoch10.pth
â”‚   â”‚   â””â”€â”€ ResNet18_CIFAR10_FT_final.pth
â”‚   â”‚
â”‚   â”œâ”€â”€ TS1_Coverage_ResNet18_CIFAR10/   # Script 2 checkpointleri
â”‚   â”‚   â”œâ”€â”€ ResNet18_CIFAR10_pruned_NC.pth
â”‚   â”‚   â”œâ”€â”€ ResNet18_CIFAR10_FTAP_NC_epoch5.pth
â”‚   â”‚   â”œâ”€â”€ ResNet18_CIFAR10_FTAP_NC_epoch10.pth
â”‚   â”‚   â”œâ”€â”€ ResNet18_CIFAR10_FTAP_NC_final.pth
â”‚   â”‚   â””â”€â”€ reports/
â”‚   â”‚       â””â”€â”€ TS1_Coverage_ResNet18_CIFAR10.pdf
â”‚   â”‚
â”‚   â””â”€â”€ TS1_Wanda_ResNet18_CIFAR10/      # Script 3 checkpointleri
â”‚       â”œâ”€â”€ ResNet18_CIFAR10_pruned_W.pth
â”‚       â”œâ”€â”€ ResNet18_CIFAR10_FTAP_W_epoch5.pth
â”‚       â”œâ”€â”€ ResNet18_CIFAR10_FTAP_W_epoch10.pth
â”‚       â”œâ”€â”€ ResNet18_CIFAR10_FTAP_W_final.pth
â”‚       â””â”€â”€ reports/
â”‚           â””â”€â”€ TS1_Wanda_ResNet18_CIFAR10.pdf
```

## ğŸš€ Ã‡alÄ±ÅŸtÄ±rma SÄ±rasÄ±

### Script 1: Model HazÄ±rlama ve Fine-Tuning
```bash
python test_scenarios/TS1_01_prepare_model.py
```

**AmaÃ§**: Pretrained ResNet-18'i indir, CIFAR-10'a uyarla ve fine-tune et.

**Ã‡Ä±ktÄ±lar**:
- Pretrained model checkpoint
- Her 5 epochta bir checkpoint (FT_epoch5, FT_epoch10, ...)
- Final fine-tuned model
- Before/After accuracy karÅŸÄ±laÅŸtÄ±rma tablosu

**Parametreler**:
- Epochs: 20
- Batch Size: 128
- Learning Rate: 0.001
- Optimizer: Adam

---

### Script 2: Neuron Coverage Pruning
```bash
python test_scenarios/TS1_02_coverage_pruning.py
```

**AmaÃ§**: Fine-tuned modele Neuron Coverage yÃ¶ntemiyle pruning uygula ve fine-tune et.

**Ã‡Ä±ktÄ±lar**:
- Pruned model checkpoint (NC)
- Her 5 epochta bir checkpoint (FTAP_NC_epoch5, ...)
- Final fine-tuned pruned model
- KapsamlÄ± karÅŸÄ±laÅŸtÄ±rma tablosu
- PDF rapor

**Parametreler**:
- Pruning Ratio: 20%
- Coverage Metric: normalized_mean
- Global Pruning: True
- Iterative Steps: 5
- Fine-Tuning Epochs: 30

**KarÅŸÄ±laÅŸtÄ±rma Tablosu Ä°Ã§eriÄŸi**:
- Accuracy (%)
- Size (MB)
- Parameters (M)
- FLOPs (G)
- Average Inference Time (ms)

---

### Script 3: WANDA Pruning
```bash
python test_scenarios/TS1_03_wanda_pruning.py
```

**AmaÃ§**: Fine-tuned modele WANDA yÃ¶ntemiyle pruning uygula ve fine-tune et.

**Ã‡Ä±ktÄ±lar**:
- Pruned model checkpoint (W)
- Her 5 epochta bir checkpoint (FTAP_W_epoch5, ...)
- Final fine-tuned pruned model
- KapsamlÄ± karÅŸÄ±laÅŸtÄ±rma tablosu
- PDF rapor

**Parametreler**:
- Pruning Ratio: 20%
- Method: WANDA (Weight Ã— Activation)
- Global Pruning: True
- Iterative Steps: 5
- Calibration Batches: 50
- Fine-Tuning Epochs: 30

**KarÅŸÄ±laÅŸtÄ±rma Tablosu Ä°Ã§eriÄŸi**:
- Accuracy (%)
- Size (MB)
- Parameters (M)
- FLOPs (G)
- Average Inference Time (ms)

---

## ğŸ“Š Beklenen SonuÃ§lar

### Script 1: Model Preparation
```
FINE-TUNING RESULTS - COMPARISON TABLE
================================================================================
Metric                         Before Fine-Tuning        After Fine-Tuning
--------------------------------------------------------------------------------
Accuracy (%)                                ~15.00                    ~92.00
Loss                                        ~2.3000                   ~0.2500
Total Parameters                        11,173,962                11,173,962
Trainable Parameters                    11,173,962                11,173,962
--------------------------------------------------------------------------------
âœ“ Accuracy Improvement: +77.00%
```

### Script 2: Coverage Pruning
```
NEURON COVERAGE PRUNING - COMPREHENSIVE COMPARISON TABLE
====================================================================================================
Metric                         Original (FT)          After Pruning       After Pruning+FT
----------------------------------------------------------------------------------------------------
Accuracy (%)                           92.00                  90.20                  91.70
Size (MB)                              42.60                  34.08                  34.08
Parameters (M)                         11.17                   8.94                   8.94
FLOPs (G)                               0.56                   0.45                   0.45
Avg Inference Time (ms)                 2.45                   2.15                   2.15
----------------------------------------------------------------------------------------------------

Summary
  Parameter Reduction                                     20.00%
  Size Reduction                                          20.00%
  Speedup                                                   1.14x
  Accuracy Recovery (FT)                                   +1.50%
  Final Accuracy Drop                                      -0.30%
```

### Script 3: WANDA Pruning
```
WANDA PRUNING - COMPREHENSIVE COMPARISON TABLE
====================================================================================================
Metric                         Original (FT)          After Pruning       After Pruning+FT
----------------------------------------------------------------------------------------------------
Accuracy (%)                           92.00                  90.80                  91.85
Size (MB)                              42.60                  34.08                  34.08
Parameters (M)                         11.17                   8.94                   8.94
FLOPs (G)                               0.56                   0.45                   0.45
Avg Inference Time (ms)                 2.45                   2.10                   2.10
----------------------------------------------------------------------------------------------------

Summary
  Parameter Reduction                                     20.00%
  Size Reduction                                          20.00%
  Speedup                                                   1.17x
  Accuracy Recovery (FT)                                   +1.05%
  Final Accuracy Drop                                      -0.15%
```

## âš™ï¸ Gereksinimler

```bash
pip install -r requirements.txt
```

**Minimum Gereksinimler**:
- Python 3.8+
- PyTorch 2.0+
- CUDA 11.0+ (GPU iÃ§in)
- 8GB+ RAM
- 10GB+ Disk Space

## ğŸ” Ä°simlendirme KurallarÄ±

### Checkpoint FormatlarÄ±

1. **Fine-Tuning Checkpoint**:
   - Format: `{Model}_{Dataset}_FT_epoch{N}.pth`
   - Ã–rnek: `ResNet18_CIFAR10_FT_epoch10.pth`

2. **Pruning SonrasÄ± (Fine-Tuning Ã–ncesi)**:
   - Format: `{Model}_{Dataset}_pruned_{Method}.pth`
   - Ã–rnek: `ResNet18_CIFAR10_pruned_NC.pth` (Neuron Coverage)
   - Ã–rnek: `ResNet18_CIFAR10_pruned_W.pth` (WANDA)

3. **Pruning + Fine-Tuning SonrasÄ±**:
   - Format: `{Model}_{Dataset}_FTAP_{Method}_epoch{N}.pth`
   - Ã–rnek: `ResNet18_CIFAR10_FTAP_NC_epoch15.pth`
   - Ã–rnek: `ResNet18_CIFAR10_FTAP_W_epoch20.pth`

### Method KÄ±saltmalarÄ±

- **NC**: Neuron Coverage
- **W**: WANDA
- **FT**: Fine-Tuning
- **FTAP**: Fine-Tuning After Pruning

## ğŸ“ Notlar

1. **GPU KullanÄ±mÄ±**: Scriptler otomatik olarak CUDA varsa GPU kullanÄ±r.

2. **Checkpoint Kaydetme**: 
   - Her 5 epochta bir model kaydedilir
   - Final model her zaman kaydedilir

3. **Memory YÃ¶netimi**:
   - Batch size GPU memory'e gÃ¶re ayarlanabilir
   - Coverage analysis iÃ§in `max_batches` parametresi kullanÄ±lÄ±r

4. **Reproducibility**:
   - Random seed scriptlerde set edilmemiÅŸtir
   - Ä°stenen sonuÃ§lar iÃ§in seed eklenebilir

5. **Fine-Tuning SÃ¼releri**:
   - Script 1: ~20-30 dakika
   - Script 2: ~40-60 dakika
   - Script 3: ~40-60 dakika

## ğŸ› Sorun Giderme

### "Fine-tuned model not found" HatasÄ±
```bash
# Ã–nce Script 1'i Ã§alÄ±ÅŸtÄ±rÄ±n
python test_scenarios/TS1_01_prepare_model.py
```

### GPU Memory HatasÄ±
```python
# Batch size'Ä± kÃ¼Ã§Ã¼ltÃ¼n
BATCH_SIZE = 64  # veya 32
```

### Dataset Ä°ndirme HatasÄ±
```python
# Manuel indirme iÃ§in
datasets.CIFAR10(root=str(DATASET_DIR), train=True, download=True)
```

## ğŸ“š Referanslar

- **WANDA Paper**: ["A Simple and Effective Pruning Approach for Large Language Models"](https://arxiv.org/abs/2306.11695)
- **Torch-Pruning**: [GitHub](https://github.com/VainF/Torch-Pruning)
- **CleanAI**: [README.md](../README.md)

## ğŸ“§ Ä°letiÅŸim

Sorular ve Ã¶neriler iÃ§in issue aÃ§abilirsiniz.

---

**Test Scenario TS1** - ResNet-18 CIFAR-10 Pruning Analysis  
*CleanAI v5 Framework*
